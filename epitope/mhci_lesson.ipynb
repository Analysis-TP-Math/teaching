{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an MHC-Class I binding predictor in Python\n",
    "## Updated September 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Peptide binding to MHC molecules is the key selection step in the Antigen-presentation pathway. This is essential for T cell immune responses. The 'epitope' is the peptide-MHC combination shown in the image at right. Key residues in the MHC contact the peptide and these differ between alleles. The prediction of peptide binding to MHC molecules has been much studied. The problem is simpler for class-I molecules since the binding peptide length is less variable (usually 8-11 but commonly 9). Typically binding predictors are based on training models with experimental binding affinity measurements with known peptide sequences. This data is available from the IEDB for many human alleles. New peptides can then be predicted based on their position specific similarity to the training data. \n",
    "\n",
    "This requires encoding the peptide amino acid sequence numerically in a manner that captures the properties important for binding. Many possible encodings have been suggested. The simplest is a so-called 'one-hot encoding' of the amino acids producing a 20-column vector for each position that only contains a 1 where the letter corresponds to the :\n",
    "\n",
    "### Peptide encoding\n",
    "\n",
    "Several encoding techniques have been proposed for representing sequence of amino acids in multidimensional metric spaces. In particular in this work we are interested in a simple encoding that is suited to be coupled with a machine learning algorithm. We will use pandas dataframes to construct the encoding, though probably not the most optimal for speed, it is convenient. First we import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 130)\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.4)\n",
    "import epitopepredict as ep\n",
    "from epitopepredict import sequtils, base, peptutils, mhclearn\n",
    "from IPython.display import display, HTML, Image\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode peptides\n",
    "\n",
    "To encode a peptide a few schemes are illustrated here. None of these methods take into account the interdependence of the amino acids in terms of their relative positions.\n",
    "\n",
    "### One hot encoding\n",
    "The first and simplest is a so-called 'one-hot encoding' of the amino acids producing a 20-column vector for each position that only contains a 1 where the letter corresponds to that amino acid.\n",
    "\n",
    "The code below uses a pandas dataframe to construct the new features. The flatten command at the end re-arranges the 2-D matrix into a 1-D format so it can be used with the regression models. This applies to the other encoding methods also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "codes = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "def show_matrix(m):\n",
    "    #display a matrix\n",
    "    cm = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "    display(m.style.background_gradient(cmap=cm))\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    o = list(set(codes) - set(seq))\n",
    "    s = pd.DataFrame(list(seq))    \n",
    "    x = pd.DataFrame(np.zeros((len(seq),len(o)),dtype=int),columns=o)    \n",
    "    a = s[0].str.get_dummies(sep=',')\n",
    "    a = a.join(x)\n",
    "    a = a.sort_index(axis=1)\n",
    "    #a = a.set_index([a.index,list(seq)])\n",
    "    #show_matrix(a)\n",
    "    e = a.values.flatten() \n",
    "    return e\n",
    "\n",
    "pep='ALDFEQEMT'\n",
    "e=one_hot_encode(pep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLF encoding \n",
    "\n",
    "This method of encoding is detailed by Nanni and Lumini in their paper. It takes many physicochemical properties and transforms them using a Fisher Transform (similar to a PCA) creating a smaller set of features that can describe the amino acid just as well. There are 19 transformed features. This is available on the github link below if you want to try it. The result is shown below for our sample peptide ALDFEQEMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#read the matrix a csv file on github\n",
    "nlf = pd.read_csv('https://raw.githubusercontent.com/dmnfarrell/epitopepredict/master/epitopepredict/mhcdata/NLF.csv',index_col=0)\n",
    "\n",
    "def nlf_encode(seq):    \n",
    "    x = pd.DataFrame([nlf[i] for i in seq]).reset_index(drop=True)  \n",
    "    #show_matrix(x)\n",
    "    e = x.values.flatten()\n",
    "    return e\n",
    "\n",
    "e=nlf_encode(pep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blosum encoding\n",
    "\n",
    "BLOSUM62 is a substitution matrix that specifies the similarity of one amino acid to another by means of a score. This score reflects the frequency of substiutions found from studying protein sequence conservation in large databases of related proteins. The number 62 refers to the percentage identity at which sequences are clustered in the analysis. Encoding a peptide this way means we provide the column from the blosum matrix corresponding to the amino acid at each position of the sequence. This produces 21x9 matrix. see https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/blosum\n",
    "\n",
    "The code to produce this array is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "blosum = ep.blosum62\n",
    "\n",
    "def blosum_encode(seq):\n",
    "    #encode a peptide into blosum features\n",
    "    s=list(seq) \n",
    "    x = pd.DataFrame([blosum[i] for i in seq]).reset_index(drop=True)\n",
    "    x = x.iloc[:,:-4]\n",
    "    #x = x.set_index([x.index,list(seq)])\n",
    "    #show_matrix(x)\n",
    "    e = x.values.flatten()\n",
    "    return e\n",
    "\n",
    "def random_encode(p):\n",
    "    return [np.random.randint(20) for i in pep]\n",
    "\n",
    "e=blosum_encode(pep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a regression model to fit encoded features\n",
    "\n",
    "We are going to create a regression model that can fit the encoded peptide featuers to known affinity data. This model is trained on the known data and then used to predict new peptides. The data used for training is primarily from the IEDB and was curated by the authors of MHCflurry from various sources. A sample of the data is shown below. The affinity measure is usually given as IC50 which is  This scale is harder for regression so we linearize the scale using the equation log50k = 1-log(ic50) / log(50000).\n",
    "\n",
    "see http://tools.iedb.org/main/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>ic50</th>\n",
       "      <th>length</th>\n",
       "      <th>log50k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15914</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>ADFKLFFRW</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15916</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>ADKNLIKCS</td>\n",
       "      <td>13201.155330</td>\n",
       "      <td>9</td>\n",
       "      <td>0.123082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15917</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>ADLRFASEF</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15919</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>ADTDSPLRY</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.679685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15921</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>AEALLADGL</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15924</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>AEFKYIAAV</td>\n",
       "      <td>15659.171326</td>\n",
       "      <td>9</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>AEFPVGSTA</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15927</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>AEFWDVFLS</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15929</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>AEGTGITHL</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15931</th>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>AEHDPWWAV</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            allele    peptide          ic50  length    log50k\n",
       "15914  HLA-A*01:01  ADFKLFFRW  20000.000000       9  0.084687\n",
       "15916  HLA-A*01:01  ADKNLIKCS  13201.155330       9  0.123082\n",
       "15917  HLA-A*01:01  ADLRFASEF  20000.000000       9  0.084687\n",
       "15919  HLA-A*01:01  ADTDSPLRY     32.000000       9  0.679685\n",
       "15921  HLA-A*01:01  AEALLADGL  20000.000000       9  0.084687\n",
       "15924  HLA-A*01:01  AEFKYIAAV  15659.171326       9  0.107300\n",
       "15925  HLA-A*01:01  AEFPVGSTA  20000.000000       9  0.084687\n",
       "15927  HLA-A*01:01  AEFWDVFLS  20000.000000       9  0.084687\n",
       "15929  HLA-A*01:01  AEGTGITHL  20000.000000       9  0.084687\n",
       "15931  HLA-A*01:01  AEHDPWWAV  20000.000000       9  0.084687"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = ep.get_training_set(allele='HLA-A*01:01')\n",
    "df=df[df.length==9]\n",
    "cols = ['allele','peptide','ic50','length','log50k']\n",
    "display(df[10:20][cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to create a regression model to fit the peptide features against the true affinity values. The code below sets up a train/test scheme for a neural network regressor using scikit-learn. scikit-learn provides many possible models that can be created with relatively little knowledge of machine learning. I chose a neural network here because it seemed most appropriate after some experimentation. The parameters used were arrived at through a grid search first. We then make scatter plots comparing the test to predicted values. The diagonal line is the ideal one to one value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit,cross_validate\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = ep.get_training_set('HLA-A*03:01', length=9)\n",
    "X = df.peptide.apply(lambda x: pd.Series(one_hot_encode(x)),1)   \n",
    "y = df.log50k\n",
    "    \n",
    "param_list = {\"hidden_layer_sizes\": [(1,),(50,)], \n",
    "              \"activation\": [\"identity\", \"logistic\"], \"max_iter\": [1500],\n",
    "              \"solver\": [\"adam\", \"lbfgs\"], \"alpha\": [0.00005,0.0005]}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "reg = MLPRegressor()\n",
    "gs = GridSearchCV(reg, param_grid=param_list)\n",
    "#gs.fit(X, y)\n",
    "#gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = dict(hidden_layer_sizes=np.random.randint(20,200,10),\n",
    "                     activation=[\"identity\", \"logistic\"],\n",
    "                     alpha=np.random.uniform(.00001,0.001,10),\n",
    "                     solver=[\"adam\", \"lbfgs\"])\n",
    "rs = RandomizedSearchCV(reg, distributions, random_state=2, n_iter=20)\n",
    "search = rs.fit(X, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def auc_score(true,sc,cutoff=.426):\n",
    "    \n",
    "    #if cutoff!=None:\n",
    "    true = (true<=cutoff).astype(int)\n",
    "    sc = (sc<=cutoff).astype(int)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true, sc, pos_label=1)\n",
    "    r = metrics.auc(fpr, tpr)\n",
    "    #print (r)\n",
    "    return  r\n",
    "\n",
    "def test_predictor(allele, encoder, ax=None):\n",
    "    \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    reg = MLPRegressor(hidden_layer_sizes=(1,), alpha=0.0005, max_iter=1500,\n",
    "                        activation='logistic', solver='lbfgs', random_state=2)\n",
    "    df = ep.get_training_set(allele, length=9)\n",
    "    print (len(df))\n",
    "    X = df.peptide.apply(lambda x: pd.Series(encoder(x)),1)\n",
    "    y = df.log50k\n",
    "    \n",
    "    res=[]\n",
    "    for i in range(3):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        #pipeline = Pipeline([('scaler', StandardScaler()), ('regressor', reg)])\n",
    "        reg.fit(X_train, y_train)    \n",
    "        sc = reg.predict(X_test)\n",
    "        auc = ep.auc_score(y_test,sc,cutoff=.426)\n",
    "        res.append(auc)\n",
    "    auc = np.mean(res)\n",
    "    \n",
    "    if ax != None:\n",
    "        x=pd.DataFrame(np.column_stack([y_test,sc]),columns=['test','predicted'])\n",
    "        x.plot('test','predicted',kind='scatter',s=8,c='black',alpha=0.6,ax=ax)\n",
    "        ax.plot((0,1), (0,1), ls=\"--\", lw=2, c=\".2\")\n",
    "        ax.set_xlim((0,1));  ax.set_ylim((0,1))\n",
    "        ax.set_title(encoder.__name__)\n",
    "        ax.text(.1,.9,'auc=%s' %round(auc,2))\n",
    "        sns.despine()\n",
    "    return auc\n",
    "\n",
    "sns.set_context('notebook')\n",
    "encs=[nlf_encode,one_hot_encode,blosum_encode,random_encode]\n",
    "allele='HLA-A*03:01'\n",
    "fig,axs=plt.subplots(2,2,figsize=(10,10))\n",
    "axs=axs.flat\n",
    "i=0\n",
    "for enc in encs:\n",
    "    print (enc)\n",
    "    test_predictor(allele,enc,ax=axs[i])\n",
    "    i+=1\n",
    "    \n",
    "plt.tight_layout()\n",
    "fig.savefig('basicmhc1_encoders.jpg',dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_alleles = ep.get_preset_alleles('mhc1_supertypes')\n",
    "res={}\n",
    "for a in m1_alleles:\n",
    "    res[a] = []\n",
    "    for enc in encs:\n",
    "        print (a,enc)\n",
    "        auc=test_predictor(a,enc)\n",
    "        res[a].append(auc)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HLA-A*01:01</th>\n",
       "      <th>HLA-A*02:01</th>\n",
       "      <th>HLA-A*03:01</th>\n",
       "      <th>HLA-A*24:02</th>\n",
       "      <th>HLA-B*07:02</th>\n",
       "      <th>HLA-B*44:03</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nlf</th>\n",
       "      <td>0.829</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one hot</th>\n",
       "      <td>0.851</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blosum</th>\n",
       "      <td>0.842</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HLA-A*01:01  HLA-A*02:01  HLA-A*03:01  HLA-A*24:02  HLA-B*07:02  HLA-B*44:03   mean\n",
       "nlf            0.829        0.867        0.833        0.751        0.860        0.746  0.814\n",
       "one hot        0.851        0.880        0.835        0.740        0.872        0.763  0.824\n",
       "blosum         0.842        0.879        0.839        0.752        0.866        0.761  0.823\n",
       "random         0.500        0.501        0.500        0.498        0.503        0.500  0.500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encdf = pd.DataFrame(res, index=['nlf','one hot','blosum','random'])\n",
    "encdf['mean'] = encdf.mean(1)\n",
    "encdf.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a predictor\n",
    "\n",
    "It can be seen from the plots that blosum_encoding gives superior results (highest auc value) for this particular test/train set and allele. Repeating the process generally shows the same outcome. Now that we know how to encode the peptides and make a model we can do it for any allele. We can write a routine that build the predictor from any training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_predictor(allele, encoder=None, length=9):\n",
    "    \"\"\"Build a regression model using peptide encoder and test data\"\"\"\n",
    "\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    #get allele specific predictor\n",
    "    if encoder==None:\n",
    "        encoder=nlf_encode\n",
    "    data = ep.get_training_set(allele, length=length)\n",
    "    \n",
    "    if len(data)<100:\n",
    "        return\n",
    "    reg = MLPRegressor(hidden_layer_sizes=(1,), alpha=0.0005, max_iter=1500,\n",
    "                        activation='logistic', solver='lbfgs', random_state=2)\n",
    "    X = data.peptide.apply(lambda x: pd.Series(encoder(x)),1)\n",
    "    y = data.log50k\n",
    "    y.hist()\n",
    "    print (allele, len(X))\n",
    "    reg.fit(X,y)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save models\n",
    "\n",
    "Now we can just use the code above for all alleles in which we have training data (>200 samples) and produce a model for each one. Each model is saved to disk for later use so we don't have to re-train every time we want to predict peptides for that allele. sklearn uses the joblib library to persist models, which is similar to the pickle module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def get_allele_names():\n",
    "    d = ep.get_training_set(length=9)\n",
    "    a = d.allele.value_counts()\n",
    "    a =a[a>200]\n",
    "    return list(a.index)\n",
    "\n",
    "def train_models():\n",
    "    alleles=['HLA-A*02:01','HLA-A*02:02']\n",
    "    path = 'models'\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "    for a in alleles:\n",
    "        fname = os.path.join(path, a+'.joblib')\n",
    "        reg = build_predictor(a, blosum_encode, 9)\n",
    "        if reg is not None:\n",
    "            joblib.dump(reg, fname, protocol=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "allele='HLA-A*02:06'\n",
    "encoder = one_hot_encode\n",
    "#reg = joblib.load('models/HLA-A*02:01.joblib')\n",
    "reg = build_predictor(allele, encoder, 9)\n",
    "reg\n",
    "\n",
    "data = mhclearn.get_evaluation_set2(allele, length=9)#[:310]\n",
    "s = data.peptide\n",
    "y = data.log50k\n",
    "y.hist()\n",
    "#print (data)\n",
    "X = s.apply(lambda x: pd.Series(encoder(x)),1)\n",
    "sc = reg.predict(X).round(4)\n",
    "auc = ep.auc_score(y,sc,cutoff=.426)\n",
    "print (auc)\n",
    "\n",
    "x=pd.DataFrame(np.column_stack([y,sc]),columns=['test','predicted'])\n",
    "fig,ax=plt.subplots(1,1,figsize=(6,6))\n",
    "x.plot('test','predicted',kind='scatter',s=20,ax=ax)\n",
    "ax.plot((0,1), (0,1), ls=\"--\", lw=2, c=\".2\")\n",
    "ax.set_xlim((0,1));  ax.set_ylim((0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to other prediction algorithms\n",
    "\n",
    "The two best MHC-class I prediction tools are currently netMHC/netMHCpan-4.0 and MHCFlurry. It is useful to compare our results to those. In order to do this I have implemented the algorithm we made (which uses the code above) as a predictor in the `epitopepredict` package. The object is created using `P = ep.get_predictor('basicmhc1')`. This standardizes the calls to prediction and the results returned so it can be directly compared to the other tools. The code below creates 3 predictor objects and evaluates their performance on some new data for each allele available. The score is then recoreded each time. Importantly, the peptides in the evaluation set are not present in the training set.\n",
    "\n",
    "Note that predictors all return an ic50 value but internally use the log50k value for fitting. The predictors are evaluated using the roc auc metric with a threshold of 500nM. The auc is a common metric for classiifcation and can be used for regression if a threshold is chosen. Though others measures may be used such as pearson correlation co-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_predictor(P, allele):\n",
    "\n",
    "    data = mhclearn.get_evaluation_set2(allele, length=9)\n",
    "    print (len(data))\n",
    "    P.predict_peptides(list(data.peptide), alleles=allele, cpus=12)\n",
    "    x = P.get_scores(allele)\n",
    "    #x = P.data\n",
    "    x = data.merge(x,on='peptide') \n",
    "    #print (x)\n",
    "    #x.plot(x='ic50',y='score',kind='scatter',s=20,)\n",
    "    #auc = auc_score(x.log50k_x,x.log50k_y,cutoff=.426)\n",
    "    auc = auc_score(x.ic50,x.score,cutoff=500)\n",
    "    return auc, data\n",
    "\n",
    "reload(base)\n",
    "reload(mhclearn)\n",
    "\n",
    "def run_tests():\n",
    "    preds = [base.get_predictor('basicmhc1'),\n",
    "             base.get_predictor('netmhcpan',scoring='affinity'),]\n",
    "             #ep.get_predictor('mhcflurry')]\n",
    "    comp=[]\n",
    "    test_alleles = mhclearn.get_allele_names()[:5]\n",
    "    omit = ['H-2-Kb']\n",
    "    for P in preds:\n",
    "        m=[]\n",
    "        for a in test_alleles:\n",
    "            if a in omit: continue\n",
    "            try:\n",
    "                auc,df = evaluate_predictor(P, a)\n",
    "                m.append((a,auc,len(df)))            \n",
    "            except Exception as e:\n",
    "                print (a,e)\n",
    "                pass\n",
    "            print (P, a, auc)\n",
    "        m=pd.DataFrame(m,columns=['allele','score','size'])\n",
    "        m['name'] = P.name\n",
    "        comp.append(m)\n",
    "    return comp\n",
    "\n",
    "comp = run_tests()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Finally we use the resultant dataframe to plot the auc scores for each method per allele. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "c=pd.concat(comp)\n",
    "x=pd.pivot_table(c,index=['allele','size'],columns='name',values='score')\n",
    "print (x)\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "#display(x.style.apply(highlight_max,1))\n",
    "#print(c)\n",
    "fig = plt.figure(constrained_layout=True,figsize=(15,10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=1)\n",
    "ax = fig.add_subplot(gs[0])\n",
    "x.plot(x='basicmhc1',y='netmhcpan',kind='scatter',s=40,c='orange',ax=ax)\n",
    "x.plot(x='basicmhc1',y='mhcflurry',kind='scatter',s=40,c='green',ax=ax)\n",
    "ax.plot((0,1), (0,1), ls=\"--\", lw=2, c=\".2\")\n",
    "ax.set_xlim(.82,.94);ax.set_ylim(.82,.94)\n",
    "ax = fig.add_subplot(gs[1])\n",
    "sns.boxplot(data=c,y='score',x='name',ax=ax)\n",
    "ax = fig.add_subplot(gs[2:])\n",
    "g=sns.barplot(data=c,y='score',x='allele',hue='name', ax=ax)\n",
    "plt.legend(bbox_to_anchor=(1.1, 1.05),fontsize=16)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "#plt.tight_layout()\n",
    "plt.savefig('basicmhc1_benchmarks.jpg', dpi=150)\n",
    "x.to_csv('benchmarks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see clearly that our method is quite inferior to the other two for many of the alleles! Our model is limited in several ways:\n",
    "\n",
    "* it only works with 9-mers\n",
    "* some alleles have preference for different peptide lengths and this is not accounted for\n",
    "* limited training data for some alleles, the other tools use methods to overcome this\n",
    "* the neural network is not likely not sophisticated enough\n",
    "\n",
    "Why the blosum62 encoding is better than the other tested is not entirely clear. However it should be better than one hot encoding since there is information loss when simply using 0 and 1 in a sparse matrix. As mentioned the predictor is available from within the epitopepredict package. It is used as a basic simple model only. To create the basic MHC predictor in epitopepredict we would use the code below. In the future this approach will be used as a basis for improving the predictor. Improvements would include handling different length peptides and changing the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using epitopepredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "P = ep.get_predictor('basicmhc1')\n",
    "from epitopepredict import peptutils\n",
    "seqs = peptutils.create_random_sequences(10)\n",
    "df = pd.DataFrame(seqs,columns=['peptide'])\n",
    "res = P.predict_peptides(df.peptide, alleles=ep.get_preset_alleles('mhc1_supertypes'), cpus=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* L. Nanni and A. Lumini, “A new encoding technique for peptide classification,” Expert Syst. Appl., vol. 38, no. 4, pp. 3185–3191, 2011.\n",
    "* V. Jurtz, S. Paul, M. Andreatta, P. Marcatili, B. Peters, and M. Nielsen, “NetMHCpan-4.0: Improved Peptide–MHC Class I Interaction Predictions Integrating Eluted Ligand and Peptide Binding Affinity Data,” J. Immunol., vol. 199, no. 9, 2017.\n",
    "* T. J. O’Donnell, A. Rubinsteyn, M. Bonsack, A. B. Riemer, U. Laserson, and J. Hammerbacher, “MHCflurry: Open-Source Class I MHC Binding Affinity Prediction,” Cell Syst., vol. 7, no. 1, p. 129–132.e4, 2018.\n",
    "* J. Hu and Z. Liu, “DeepMHC : Deep Convolutional Neural Networks for High-performance peptide-MHC Binding Affinity Prediction,” bioRxiv, pp. 1–20, 2017.\n",
    "* https://www.iedb.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
